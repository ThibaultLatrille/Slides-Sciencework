\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{fullpage}

\author{Thibault Latrille \and Yiming Zhang}
\title{Time series, Home assignment 2}  


\begin{document}
\maketitle

\subsubsection*{Problem 1}
\

We express the MA(1) process in the following form:
\[
X_{t}=(1+\theta B)Z_{t}=(1+0.6B)Z_{t}
\]


where $\theta=0.6$, $B$ is the standard backshift operator with $B^{j}X_{t}=X_{t-j}$
, $X_{t}$ has zero mean and $Z_{t}$ is the white-noise porcess.
In order for the process to be invertible. The root of $1+0.6B=0 \iff B=\dfrac{-5}{3}$
must lie outside the unit circle. It is straightforward in this case and the process is invertible.
\\


To simulate our process, we wrote our own code instead of using the in-built function \emph{arima.sim} (because it is funnier).

\begin{verbatim}
###### R-code #########
MA<-function(n,theta){
X<-rep(0,n) 
Z<-rnorm(n,0,1) # White noise process Z
X[1]<-Z[1]
for (j in 2:n){
	X[j]<-theta*Z[j-1]+Z[j] # MA(1) process using the Z process
	}
return(ts(X))
}
theta<-0.6
X<-MA(1000,theta)
plot(X)
\end{verbatim}

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.50\textwidth]{TS.png}
  	\caption{\textbf{Time series plot.} }
\end{figure}


For the MA(1) process $X_{t}=(1+\theta B)Z_{t}$, we have $\rho_1=\dfrac{\theta}{1+\theta^2}$ and $\rho_k=0$ for $k>1$, which leads to $\theta = \dfrac{1- \sqrt{1-4 \rho_1^2}}{2\rho_1}$ (proof below). Thus we have the plug-in estimator $\widehat{\theta} = \dfrac{1 - \sqrt{1-4 \widehat{\rho}_1^2}}{2\widehat{\rho}_1}$, where $\widehat{\rho}_1$ is the sample auto-correlation function for lag 1.
\\


In order to get the distribution of the estimators, we simulated the process 100 times and computed the estimates for every simulation. 
\begin{verbatim}
###### R-code #########
plugin<-rep(0,100) # vector of Plug-in estimators 
CSS_ML<-rep(0,100) # vector of Maximum Likelihood estimators
CSS<-rep(0,100)    # vector of Conditional Sum of Squares estimators 
for (j in 1:100){
	X<-MA(1000,theta) # simulating the MA process
	rho<-acf(X,plot=FALSE)[[1]][2] # computing the sample auto-correlation function for lag 1
	if ((1-4*rho^2)<0){plugin[j]<-1/(2*rho)}
	else {plugin[j]<-1/(2*rho)-sqrt(1-4*rho^2)/(2*rho)}# computing the plug-in estimator
	CSS_ML[j]<-arima(X,order=c(0,0,1),method="ML")$coef[[1]] # ML
	CSS[j]<-arima(X,order=c(0,0,1),method="CSS")$coef[[1]] # CSS
}
boxplot(list(plugin,CSS_ML,CSS))
\end{verbatim}

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{boxplotMA1.png}
  	\caption{\textbf{Box-plot of the parameter estimation for different estimators.} The red line is the underlying parameter, we can clearly witness that our plug-in estimator has an higher variance, the built-in estimators are better.}
\end{figure}

We use the box-plot command to compare the ML estimators for different sample lengths.
\begin{verbatim}
###### R-code #########
N<-c(100,200,300,400,500,600,700,800,900,1000)
D=data.frame(N=rep(N,rep(100,10)),estimator=rep(10*100)) # data-frame table 
i<-0
for (n in N){
for (j in 1:100){
	X<-MA(n,theta)
	D$estimator[i*100+j]<-arima(X,order=c(0,0,1),method="CSS-ML")$coef[[1]]
}
i<-i+1
}
boxplot(estimator~N,D)
\end{verbatim}

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{boxplotMA1N.png}
  	\caption{\textbf{Box-plot of the parameter estimations for different N.} The red line is the underlying parameter, the variance decreases as the sample length increases.}
\end{figure}

To find the theoretical ACF of the process, we multiply both sides of the process
by $X_{t-k}$ and take the expectation
\[
\gamma_k=EX_{t}X_{t-k}=EZ_{t}Z_{t-k}+\theta EZ_{t-1}Z_{t-k}+\theta EZ_{t}Z_{t-k-1}+\theta^2EZ_{t-1}Z_{t-k-1}
\]
$$
\gamma_k=
\left\{
\begin{array}{cc}
1+\theta^2  & 
k=0  \\
\theta & 
k=1  \\
0 & 
k>1  
\end{array}
\right.
\iff
\rho_k=
\left\{
\begin{array}{cc}
 1 & k=0 \\
 \dfrac{\theta}{1+\theta^2} & k=1\\
 0 & k>1
    \end{array}
\right. 
$$

\begin{verbatim}
###### R-code #########
acfx<-acf(X)
gamma2.ar<-rep(0,31)
gamma2.ar[1]<-1
gamma2.ar[2]<-theta/(1+theta**2)
points(seq(0,30,1),gamma2.ar,"p",col=2,lwd=3,pch=19)
\end{verbatim}


\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{acf.png}
  	\caption{\textbf{Sample and theoretical autocorrelation function for N=1000} }
\end{figure}

\subsubsection*{Problem 2}
We express the MA(2) process in the following form:
\[
X_{t}=(1+\theta_1B+\theta_2 B^2)Z_{t}=(1-0.5B+0.6 B^2)Z_{t}
\]

In order for the process to be invertible. The roots of $1-0.5B+0.6 B^2=0$
must lie outside the unit circle. The roots are 
$\dfrac{5 \pm i \sqrt{215}}{12}
$ with norm equal $ \sqrt{\dfrac{5}{3}}$. Thus the process is invertible.

\begin{verbatim}
###### R-code #########
MA2<-function(n,theta1,theta2){
	X<-rep(0,n)
	Z<-rnorm(n,0,1) # White noise process Z
	X[1]<-Z[1]
	X[2]<-theta1*Z[1]+Z[2]
	for (j in 3:n){
		X[j]<-Z[j]+theta1*Z[j-1]+theta2*Z[j-2] #MA(2) process using the Z process
	}
	return(ts(X))
}
theta1<-(-0.5)
theta2<-(0.6)
X<-MA2(1000,theta1,theta2)
plot(X)
\end{verbatim}

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{2TS.png}
  	\caption{\textbf{Time series plot.} }
\end{figure}

In order to get the distribution of the estimators, we simulated the process 100 times and computed the estimates for every simulation. 

\begin{verbatim}
###### R-code #########
THETA1<-rep(0,100)
THETA2<-rep(0,100)
for (j in 1:100){
	X<-MA2(1000,theta1,theta2)
	THETA1[j]<-arima(X,order=c(0,0,2))$coef[[1]]
	THETA2[j]<-arima(X,order=c(0,0,2))$coef[[2]]
}
boxplot(list(THETA2))
\end{verbatim}

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{2boxplot.png}
  	\caption{\textbf{Box-plot of the parameters estimation.} The red line is the underlying parameters, the first box-plot is $\theta_1$, the second box-plot is $\theta_2$.}
\end{figure}


To find the theoretical ACF of the process, we multiply both sides of the process
by $X_{t-k}$ and take the expectation
\[
\gamma_k=EX_{t}X_{t-k}=E(1+\theta_1Z_{t-1}+\theta_2 Z_{t-2})(1+\theta_1Z_{t-k-1}+\theta_2 Z_{t-k-2})
\]
$$
\gamma_k=
\left\{
\begin{array}{cc}
1+\theta^2+\theta_2^2 & k=0 \\
 \theta_1(1+\theta_2) & k=1\\
 \theta_2 & k=2\\
 0 & k>2
\end{array}
\right.
\iff
\rho_k=
\left\{
\begin{array}{ccccc}
 1 & k=0 \\
 \dfrac{\theta_1(1+\theta_2)}{1+\theta^2+\theta_2^2} & k=1\\
 \dfrac{\theta_2}{1+\theta^2+\theta_2^2} & k=2\\
 0 & k>2
    \end{array}
\right. 
$$

\begin{verbatim}
###### R-code #########
acfx<-acf(X)
gamma2.ar<-rep(0,31)
gamma2.ar[1]<-1
gamma2.ar[2]<-theta1*(theta2+1)/(1+theta1**2+theta2**2)
gamma2.ar[3]<-theta2/(1+theta1**2+theta2**2)
points(seq(0,30,1),gamma2.ar,"p",col=2,lwd=3,pch=19)
\end{verbatim}


\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{2acf.png}
  	\caption{\textbf{Sample and theoretical autocorrelation function for a sample size of 1000} }
\end{figure}

\subsubsection*{Problem 3}


\subparagraph*{1)}

We express the AR(2) process in the following form:
\[
(1-\phi_{1}B-\phi_{2}B^{2})X_{t}=Z_{t}
\]


In order for the process to be stationary. The roots of $\phi(B)=(1-\phi_{1}B-\phi_{2}B^{2})=0$
must lie outside the unit circle. Therefore, we have
\[
|B_{i}|=|\frac{-\phi_{1}\pm\sqrt{\phi_{1}^{2}+4\phi_{2}}}{2\phi_{2}}|\geq1
\]


Solving the above, we get the conditions
\[
\begin{cases}
\phi_{2}+\phi_{1}<1\\
\phi_{2}-\phi_{1}<1\\
-1<\phi_{2}<1
\end{cases}
\]


Therefore, for the process
\[
X_{t}=X_{t-1}+cX_{t-2}+Z_{t}
\]
\[
(1-B-cB^{2})X_{t}=Z_{t}
\]


We have that the process is stationary when the roots of $cB^{2}+B-1=0$
lies outside the unit circle, therefore using the conditions stated
above, we must have
\[
\begin{cases}
c+1<1\\
c-1<1\\
-1<c<1
\end{cases}
\]


Therefore $c$ must satisfy $-1<c<0$.

To find the ACF of the process, we multiply both sides of the process
by $X_{t-k}$ and take the expectation
\[
EX_{t}X_{t-k}=EX_{t-1}X_{t-k}+cEX_{t-2}X_{t-k}+EZ_{t}X_{t-k}
\]
\[
\rho_{k}=\rho_{k-1}+c\rho_{k-2}
\]


Solving the above difference equation, we get
\[
\rho_{k}=\begin{cases}
b_{1}[\frac{1+\sqrt{1+4c}}{2}]^{k}+b_{2}[\frac{1-\sqrt{1+4c}}{2}]^{k} & 1+4c\ne0\\
(b_{1}+b_{2}k)(\frac{1}{2})^{k} & 1+4c=0
\end{cases}
\]


For $k=1$ and $2$, we have the initial conditions
\[
\rho_{1}=1+c\rho_{1}
\]
\[
\rho_{2}=\rho_{1}+c
\]


which implies that
\[
\rho_{1}=\frac{1}{1-c}
\]
\[
\rho_{2}=\frac{1}{1-c}+c
\]


Using these initial conditions and $c=-\frac{3}{16}$, and plugging
into the solutions of the difference equation, we have
\[
\begin{cases}
\frac{3}{4}b_{1}+\frac{1}{4}b_{2}=\frac{16}{19}\\
\frac{9}{16}b_{1}+\frac{1}{16}b_{2}=\frac{199}{304}
\end{cases}
\]


Solving the above, we have $b_{1}=\frac{45}{38}$, $b_{2}=-\frac{7}{38}$.
Therefore, we have the ACF for the process is
\[
\rho_{k}=\frac{45}{38}(\frac{3}{4})^{k}-\frac{7}{38}(\frac{1}{4})^{k}
\]



\subparagraph*{2)}

The process can be expressed as
\[
(1-B-cB^{2}+cB^{3})X_{t}=Z_{t}
\]


The solutions of the
equation $cB^{3}-cB^{2}-B+1=0$ must all lie outside the unit circle
in order for the process to be stationary. However since
\[
cB^{3}-cB^{2}-B+1=cB^{2}(B-1)-(B-1)=(B-1)(cB^{2}-1)=0
\]


Therefore at least one of the roots for the equation is $B=1$ hence
the process is not stationary.


\subsubsection*{Problem 4}


\subparagraph*{1)}

Using the arguments made in problem 3, part 1, $\alpha_{1}$, $\alpha_{2}$
must satisfy
\[
\begin{cases}
\alpha_{2}+\alpha_{1}<1\\
\alpha_{2}-\alpha_{1}<1\\
-1<\alpha_{2}<1
\end{cases}
\]


in order for the process to be stationary.


\subparagraph*{2)}

Using similar arguments made in problem 3, part 1, to find the ACF
of the process, we multiply both sides of the process by $X_{t-k}$
and take the expectation
\[
EX_{t}X_{t-k}=\alpha_{1}EX_{t-1}X_{t-k}+\alpha_{2}EX_{t-2}X_{t-k}+EZ_{t}X_{t-k}
\]
\[
\rho_{k}=\alpha_{1}\rho_{k-1}+\alpha_{2}\rho_{k-2}
\]


For $k=1$ and $2$, we have the initial conditions
\[
\rho_{1}=\alpha_{1}+\alpha_{2}\rho_{1}
\]
\[
\rho_{2}=\alpha_{1}\rho_{1}+\alpha_{2}
\]


which implies that
\[
\rho_{1}=\frac{\alpha_{1}}{1-\alpha_{2}}
\]
\[
\rho_{2}=\frac{\alpha_{1}^{2}}{1-\alpha_{2}}+\alpha_{2}
\]


Solving the above difference equation, we get
\[
\rho_{k}=\begin{cases}
b_{1}[\frac{\alpha_{1}+\sqrt{\alpha_{1}^{2}+4\alpha_{2}}}{2}]^{k}+b_{2}[\frac{\alpha_{1}-\sqrt{\alpha_{1}^{2}+4\alpha_{2}}}{2}]^{k} & \alpha_{1}^{2}+4\alpha_{2}\ne0\\
(b_{1}+b_{2}k)(\frac{\alpha_{1}}{2})^{k} & \alpha_{1}^{2}+4\alpha_{2}=0
\end{cases}
\]


Where $b_{1}$ and $b_{2}$ can be solved with the inital conditions.


\subparagraph*{3)}
\

\begin{figure}[H]
	\centering
  	\includegraphics[width=0.6\textwidth]{Rplots.pdf}
  	\caption{\textbf{Sample and theoretical autocorrelation function of the AR(2) model for N=100} }
\end{figure}


\subparagraph*{4)}

We will simulate an AR(2) series $X_{t}=0.4X_{t-1}-0.6X_{t-2}+Z_{t}$
where $Z_{t}$ is the white noise process using R.

R Codes

ar2.sim <- arima.sim(model=list(ar=c(0.4, -0.6)), n=100)


\subparagraph*{5)}

We will plot the time series using the following R codes.

R Codes

plot(ar2.sim)
\includegraphics[width=0.60\textwidth]{plot-ar2.pdf}

\subparagraph*{6)}

We will calculate the sample ACF with the following R codes

R Codes

acf(ar2.sim)
\includegraphics[width=0.60\textwidth]{plot-acfar2.pdf}

\subparagraph*{7)}

We will calculate the sample PACF with the following R Codes

R Codes

pacf(ar2.sim)
\includegraphics[width=0.60\textwidth]{plot-pacfar2.pdf}


\subparagraph*{8)}

We will estimate the parameters with the following R Codes

R Codes

arima(ar2.sim, order=c(2,0,0))

The estimated coefficients are
\begin{table}
\centering%
\begin{tabular}{|c|c|c|c|}
\hline 
 & $\phi_{1}$ & $\phi_{2}$ & Intercept\tabularnewline
\hline 
\hline 
 & 0.4193 & -0.5295 & -0.0458\tabularnewline
\hline 
S.E. & 0.0849 & 0.0843 & 0.0937\tabularnewline
\hline 
\end{tabular}

\caption{Estimated Coefficients for the Simulated AR(2) Series}


\end{table}


Compared with the theoretical values of $\phi_{1}=0.4$, $\phi_{2}=-0.6$,
the estimated coefficients are quite satisfactory.

\paragraph{5)}
\subparagraph{1)}
\

a) $\nabla_3 X_t=(1-B^3)X_t=f(t)+S_t+Z_t- (f(t)+S_{t-3}+Z_{t-3})=-4+3t+S_t+Z_t- (-4+3(t-1)+S_{t}+Z_{t-3})=9+Z_t-Z_{t-3}$, which is a stationary moving average process.
\\

b) $\nabla^2+2B=(1-B)^2+2B=1-2B+B^2+2B=1+B^2=-(1-B^2)+2=\nabla_2+2$
\\

c) $\nabla_{12}X_t=(1-B^{12})X_t=f(t)+S_t+Z_t- (f(t-12)+S_{t-12}+Z_{t-12})=-4+2t+S_t+Z_t-(-4+2t-24+S_{t}+Z_{t-12})=24+Z_t-Z_{t-12}$, which is a stationary moving average process.
\\

d)$\nabla_{6}X_t=(1-B^{6})X_t=f(t)+S_t+Z_t- (f(t-6)+S_{t-6}+Z_{t-6})=2t^2+S_t+Z_t-(2(t-6)^2+S_{t}+Z_{t-6})=-36+12t+Z_t-Z_{t-6}$, which is not stationary.

c) $\nabla_{12}X_t=(1-B^{12})X_t=f(t)+S_t+Z_t- (f(t-12)+S_{t-12}+Z_{t-12})$\\$=2$sin$(-\dfrac{\pi}{6}t)+S_t+Z_t-(2$sin$(-\dfrac{\pi}{6}t+2\pi)+S_{t}+Z_{t-12})=2$sin$(-\dfrac{\pi}{6}t)+Z_t-2$sin$(-\dfrac{\pi}{6}t)-Z_{t-12})=Z_t-Z_{t-12}$, which is a stationary moving average process.
\\

\subparagraph{2)}
a) We express the AR(2) process in the following form:
\[
(1+11\alpha B+10\alpha^2 B^{2})X_{t}=Z_{t}
\]


In order for the process to be stationary, the roots of $\phi(B)=1+11\alpha B+10\alpha^2 B^{2}=0$
must lie outside the unit circle. Therefore, we have
\[
\left\{
\begin{array}{cc}
& |B_{1}|=\left| \dfrac{-1}{\alpha} \right|>1 \\
& |B_{2}|=\left| \dfrac{-1}{10\alpha} \right|>1\\
\end{array}
\right.
\iff \alpha<0.1
\]

b) The Yule-Walker equations, under the assumption that the process is stationary are :

\[
\rho_j=-11\alpha\rho_{j-1}-10\alpha^2\rho_{j-2} +\delta_j, \quad j\in \mathbb{N}
\]

c) Taking the Yule-Walker equations for $j=3$ and replcaing the value by the estimates, we get

\[
\widehat{r_3} =-11\widehat{\alpha} \widehat{r_2}-10\widehat{\alpha}^2 \widehat{r_1} \\
\iff \widehat{\alpha}=\dfrac{-11\widehat{r_2} \pm \sqrt{121\widehat{r_2}^2-40\widehat{r_1}\widehat{r_2}}}{20\widehat{r_1}}
\]

Thus the estimator of $\alpha$ using the Yule-Walker equation is not uniquely defined and can be undefined, depending on the sign of $121\widehat{r_2}^2-40\widehat{r_1}\widehat{r_2}$

d) Using the same method as in exercise 3, we get for lag 1 and 2

\[
\rho_{1}=\frac{-11\alpha}{1+10\alpha^2}, \quad 
\rho_{2}=\frac{121\alpha^{2}}{1+10\alpha^2}-10\alpha^2
\]

And for $k \geq 3$ , we then have 
\[
\rho_{k}=
b_{1}[\frac{-11\alpha+\sqrt{121\alpha^{2}-40\alpha^2}}{2}]^{k}+b_{2}[\frac{-11\alpha-\sqrt{121\alpha^{2}-40\alpha^2}}{2}]^{k}=b_{1}[-\alpha]^{k}+b_{2}[-10\alpha]^{k}
\]

Where $b_{1}$ and $b_{2}$ can be solved with the given value of $\rho_{1}$ and $\rho_{2}$.

e) The partial autocorrelation function equal $-10\alpha^2$ at lag $2$ and vanishes afterward, therefore equal $0$ at lag $10$.

\subparagraph{3)}

a) We express the AR(3) process in the following form:
\[
(1-\dfrac{1}{3}B-\dfrac{1}{4} B^{2}+\dfrac{1}{12}B^3)X_{t}=Z_{t}
\]


In order for the process to be stationary. The roots of $\phi(B)=1-\dfrac{1}{3}B-\dfrac{1}{4} B^{2}+\dfrac{1}{12}B^3=0$
must lie outside the unit circle. The roots of this polynomial are $3$ and $\pm2$, therefore the process is stationary. \\

b) To be a linear forecast, the estimator $\widehat{X}_{N+k}$ must be a linear function of the observed $X_1, \hdots,X_N $. 
Thus $\displaystyle \widehat{X}_{N+k}=\sum_{i=1}^N a_i X_{N-i+1}$.
 Moreover, the property of the best linear forecast is such that $(a_i)$ $i\geq 1$ must minimize the mean squared error $E [ (\widehat{X}_{N+k}-X_{N+k})^2 ] $.\\
 
c) The best linear forecast satisfies $\dfrac{\partial E [ (\widehat{X}_{N+k}-X_{N+k})^2 ]}{\partial a_i}=0$ for $i\geq 1$. 
Which leads to the equation $Cov(\widehat{X}_{N+k}-X_{N+k},X_i)=0$ for $i\geq 1$. Thus $\widehat{X}_{N+k}$ is the orthogonal projection of $X_{N+k}$ on the Hilbert space spanned by $X_1, \hdots,X_N $, with the scalar product defined as $ \langle X,Y\rangle =E[XY]$.\\

It is well known that the orthogonal projection of ${X}_{N+k}$ on the Hilbert space spanned by $X_1, \vdots,X_N $ is the expectation of $X_N$ conditional to $X_1, \hdots,X_N $. 
Thus $\widehat{X}_{N+k}=E(X_{N+k} / X_1, \hdots,X_N$

d) For the best linear forecast, we forget about the white noise process because the forecast is blind to the white noise since, there is no information carried by the noise. The recurrence relation for the process becomes $X_t=\dfrac{1}{3}X_{t-1}-\dfrac{1}{4}X_{t-2}+\dfrac{1}{12}X_{t-3}$. 
\begin{align*}
\text{Define }
\pmb{X_{t}}=
\begin{pmatrix}
X_{t-2} \\
X_{t-1} \\
X_{t}
\end{pmatrix}
\qquad
\pmb{A}=
\begin{pmatrix}
0&1&0 \\
0&0&1 \\
\dfrac{-1}{12} & \dfrac{1}{4} & \dfrac{1}{3} \\
\end{pmatrix}
\end{align*}

And the recurrence relation become $\pmb{X_{t+1}}=\pmb{A X_{t}}$. And by recurrence $\pmb{X_{t+k}}=\pmb{A^k X_{t}}$.

It is then straightforward to compute the best linear forecast for $X_{t+1}$, $X_{t+3}$ and $X_{t+5}$. \\

$X_{t+1}=\dfrac{1}{3}X_{t}-\dfrac{1}{4}X_{t-1}+\dfrac{1}{12}X_{t-2}$
\begin{align*}
\begin{pmatrix}
X_{t+3} \\
X_{t+4} \\
X_{t+5}
\end{pmatrix}
=
\begin{pmatrix}
0&1&0 \\
0&0&1 \\
\dfrac{-1}{12} & \dfrac{1}{4} & \dfrac{1}{3} \\
\end{pmatrix}^5
.
\begin{pmatrix}
X_{t-2} \\
X_{t-1} \\
X_{t}
\end{pmatrix}=
\begin{pmatrix}
 -\frac{13}{432} & \frac{1}{16} & \frac{13}{108} \\
 -\frac{13}{1296} & 0 & \frac{133}{1296} \\
 -\frac{133}{15552} & \frac{1}{64} & \frac{133}{3888} \\
\end{pmatrix}
.
\begin{pmatrix}
X_{t-2} \\
X_{t-1} \\
X_{t}
\end{pmatrix}
\end{align*}

e)\begin{align*}
\begin{pmatrix}
X_{14} \\
X_{13} \\
X_{15}
\end{pmatrix}
=
\begin{pmatrix}
 -\frac{13}{432} & \frac{1}{16} & \frac{13}{108} \\
 -\frac{13}{1296} & 0 & \frac{133}{1296} \\
 -\frac{133}{15552} & \frac{1}{64} & \frac{133}{3888} \\
\end{pmatrix}
.
\begin{pmatrix}
X_{8} \\
X_{9} \\
X_{10}
\end{pmatrix}
=
\begin{pmatrix}
0.128125 \\
0.030625 \\
0.0328646
\end{pmatrix}
\end{align*}
\subparagraph{4)}
Using the same method as we did in exercise 1, we get 
$$
\rho_k=
\left\{
\begin{array}{cc}
 1 & k=0 \\
 \dfrac{1/4}{1+(1/4)^2}=\dfrac{4}{17} & k=1\\
 0 & k>1
    \end{array}
\right. 
$$
One can prove that the theoretical PACF at lag 1 equal $\rho_1=\dfrac{4}{17}$ and equal $\dfrac{\rho_1^2}{1-\rho_1^2}=\dfrac{16}{273}$ at lag 2.
\end{document}
