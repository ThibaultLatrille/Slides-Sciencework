\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{fullpage}

\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=4,
	rulecolor=,
	language=matlab,
        basicstyle=\scriptsize,
        upquote=true,
        aboveskip={1.5\baselineskip},
        columns=fixed,
        showstringspaces=false,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
}
\author{Latrille Thibault\\
\small thibault.latrille@ens-lyon.fr\\[-0.8ex]
\small Uppsala Universitet\\}

\title{Home Assignment 3, Markov Processes}
\begin{document}

\maketitle
\paragraph{Exercise I.\\}
\subparagraph{a)}\
$\forall n \geq 0$\\
If the chain is at state $0$, we have the transition probabilities 
$$
\displaystyle \left \{
    \begin{array}{l}
        P(X_{n+1}=0|X_{n}=0)=p \\
		P(X_{n+1}=2|X_{n}=0)=1-p \\
    \end{array}
\right.
$$
And if the chain is at state $i$ ($\forall i \geq 1$), we then have the transition probabilities 
$$
\displaystyle \left \{
    \begin{array}{l}
        P(X_{n+1}=i-1|X_{n}=i)=p \\
		P(X_{n+1}=i+2|X_{n}=i)=1-p \\
    \end{array}
\right.
$$

\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.65\textwidth]{HA3_03.png}
  	  	\caption{\textbf{The depicted diagram of markov chain.}}
	\end{figure}
With the diagram, one can see clearly that the chain is not irreducible for $p=1$ since it is impossible to jump upward. The case $p=0$ leads also to a reducible Markov chain since it splits up the state space into odd and even numbers. For $0<p<1$, the chain is irreducible since any state can reach any other state. And the chain is also aperiodic in this case.
\subparagraph{b)}
\
For $i>0$, let $S_i=X_{n+1}-X_{n}|X_n=i$ ($\forall n\geq 0$).\\
Then 
$E(S_i)=E[X_{n+1}-X_{n}|X_n=i]=2P(X_{n+1}-X_n=2|X_n=i)-P(X_{n+1}-X_n=-1|X_n=i)=2(1-p)-p=2-3p$. The Markov chain has an equilibrium distribution if $E(S_i)<0\iff p>2/3$. Indeed if $p<2/3$, every jump will go upward on average, and since the state space is infinite, the chain will diverge. 
\subparagraph{c)}
\
The transition Matrix is 
\begin{align*}
\mathbb{P}=
\begin{pmatrix}
p & 0 & 1-p & 0 & 0 & 0 &\cdots  \\
p & 0 & 0 & 1-p & 0 & 0 & \\
0 & p & 0 & 0 & 1-p & 0 & \\
0 & 0 & p & 0 & 0 & 1-p &  \\
\vdots &  &  & \ddots & &  & \ddots  \\
\end{pmatrix}
\end{align*}
For $p>2/3$, we seek to find $\pi$ such that $\pi=\pi\mathbb{P}$

\begin{gather*}
p \pi_0+ p \pi_1=\pi_1 \iff \pi_1=\frac{p \pi_0}{1-p} \\
p \pi_2=\pi_1 \iff \pi_2=\dfrac{\pi_1}{p}=\frac{\pi_0}{1-p}\\
\pi_{i+2}=p \pi_{i+3}+(1-p)\pi_{i}, \quad \forall i\geq 0
\end{gather*}
The sequence defined by the recursive rule leads to the characteristic polynomial $\pmb{P}[\lambda]=\lambda^3-\dfrac{\lambda}{p^2}-\dfrac{1-p}{p}$.
This polynomial if of degree three and hence has three roots, one real and two imaginary conjugated roots, but they are quite complicated as pointed out by mathematica.
\begin{figure}[H]
	  \centering
  	\includegraphics[width=1\textwidth]{lambda.png}
  	  	\caption{\textbf{The roots of $\pmb{P}[\lambda]=\lambda^3-\dfrac{\lambda^2}{p}-\dfrac{1-p}{p}$ solved by mathematica.}}
	\end{figure}

Since the analytic solution of $\pi_0$ is complicated, we rather prefer to use the computer power to get an approximation of $\pi_0$. Indeed $\pi_0 = 1/E(T_0)$ where the random variable $T_0$ is the number of steps before the chain goes back to state $0$, starting at $0$.
It is straightforward to write a Monte-Carlo algorithm simulating a chain starting at state $0$ and looping until it goes back to $0$. \\

We used the law of large number to get an approximation of $\displaystyle \pi_0 \approx \sum_{i=1}^n \dfrac{\pi_0^{(i)}}{n}$, where $\pi_0^{(i)}$ are independent simulation of $\pi_0$.\\

 We bootstraped (repeated) this procedure to get a $95\%$ confidence interval for the estimate of $\pi_0$.
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.9\textwidth]{bootstrap.png}
  	  	\caption{\textbf{The Monte-Carlo simulation of $\pi_0$ as a function of the parameter $p$.} The Scilab code is enclosed to this document, see appendix I.}
	\end{figure}











\paragraph{Exercise II.\\}
\subparagraph{1)}
\
For $\widehat{\pmb{P}}$ to be the transition matrix of a Markov chain, it must fulfill the requirement $\displaystyle \sum_{j=1}^{|S|} \widehat{p_{ij}} =1$ for every $i \in S$.

\begin{align*}
\displaystyle \sum_{j=1}^{|S|} \widehat{p_{ij}}=\sum_{j=1}^{|S|} \dfrac{\pi_j p_{ij}}{\pi_i}=\dfrac{1}{\pi_i}\sum_{j=1}^{|S|} \pi_j p_{ij}=\dfrac{1}{\pi_i}(\pi.\pmb{P})_i=\dfrac{1}{\pi_i}(\mathbb{\pi})_i=\dfrac{\pi_i}{\pi_i}=1\text{ since } \pi . \pmb{P}=\pi \text{ for the stationnay distribution }\pi.
\end{align*}
This is true for every $i \in S$ and $\widehat{\pmb{P}}$ is the transition matrix of a Markov chain. 
\subparagraph{b)}

To find the stationary distribution, we solve $\pi \mathbb{P}=\pi$ with  $\pi_1+\pi_2+\pi_3=1$.
 
\begin{gather*}
(1-\alpha) \pi_1+\gamma\pi_3=\pi_1 \iff \pi_3=\frac{\alpha \pi_1}{\gamma} \\
\alpha \pi_1+(1-\beta)\pi_2=\pi_2 \iff \pi_2=\frac{\alpha \pi_1}{\beta} \\
\pi_1+\pi_2+\pi_3=1 \iff \pi_1(1+\dfrac{\alpha}{\beta}+\dfrac{\alpha}{\gamma})=1 \iff \pi_1=\frac{\beta \gamma}{\alpha \beta +\alpha \gamma+\beta \gamma} 
\end{gather*}
Thus the stationary distribution is :
$$\pi=\left( \frac{\beta \gamma}{\alpha \beta +\alpha \gamma+\beta \gamma},\frac{\alpha \gamma}{\alpha \beta +\alpha \gamma+\beta \gamma},\frac{\alpha \beta }{\alpha \beta +\alpha \gamma+\beta \gamma}\right)$$

\begin{gather*}
\Rightarrow 
\widehat{\pmb{P}}= 
\begin{pmatrix}
(1-\alpha)\frac{\beta \gamma}{\beta \gamma} & 0 & \gamma\frac{\beta \alpha}{\beta \gamma}  \\
\alpha\frac{\beta \gamma}{\alpha \gamma} & (1-\beta)\frac{\alpha \gamma}{\alpha \gamma} & 0 \\
0 & \beta \frac{\alpha \gamma}{\beta \alpha} & (1-\gamma)\frac{\alpha \beta }{\alpha \beta }   \\
\end{pmatrix}
 = 
\begin{pmatrix}
1-\alpha & 0 & \alpha  \\
\beta & 1-\beta & 0 \\
0 & \gamma & 1-\gamma   \\
\end{pmatrix}
\end{gather*}
\subparagraph{c)}
We clearly have $\widehat{\pmb{P}} \neq \pmb{P}$, so the detailed balance equations are not fulfilled.














\paragraph{Exercise III.\\}
For $t_1<t_2$, and $\{ B(t), t\geq 0 \}$ a standard Brownian motion we have :
\begin{align*}
P\left( B(t_1)<B(t_2) \right) &= P\left( B(t_2)-B(t_1)>0 \right) \\
 &=P\left(\mathcal{N}(0,t_2-t_1)>0 \right) \\
&=\int_{0^+}^{+\infty}\dfrac{1}{\sqrt{2\pi (t_2-t_1)}}e^{-\dfrac{x^2}{2(t_2-t_1)}} \\
&=\dfrac{1}{2}
\end{align*}
For $t_1<t_2<\cdots<t_n$, we can then generalize the previous relation :
\begin{align*}
P\left( B(t_1)<B(t_2)<\cdots<B(t_n) \right) &= P\left( \bigcap_{i=1}^{n-1} B(t_i)<B(t_{i+1})\right) \\
 &=\prod_{i=1}^{n-1} P\left(  B(t_i)<B(t_{i+1})\right)\text{ by independence of the increments.} \\
&=\prod_{i=1}^{n-1} \dfrac{1}{2}\text{ cf above.} \\
&=\dfrac{1}{2^{n-1}}
\end{align*}















\paragraph{Exercise IV.\\}
\subparagraph{1)}
\setlength{\jot}{8pt}
\begin{align*}
N E\left[ Y_N(t+1/N)-Y_N(t) | Y_N(t)=y\right] &=N E\left[ N^{-1} X_{[N(t+1/N)]}-N^{-1} X_{[Nt]} | N^{-1} X_{[Nt]}=y\right] \\
&=  E\left[ X_{[Nt]+1}-X_{[Nt]} | X_{[Nt]}=i\right]\text{ where }i=yN \\
&= P\left( X_{[Nt]+1}-X_{[Nt]}=1| X_{[Nt]}=i\right)-P\left( X_{[Nt]+1}-X_{[Nt]}=-1| X_{[Nt]}=i\right) \\
&= i.v-i.u=i(v-u)\text{ by definition of the process.} \\
\end{align*}
The expected value of the difference is :
$$ E\left[ Y_N(t+1/N)-Y_N(t) | Y_N(t)=y\right]=y(v-u)
$$
\setlength{\jot}{8pt}
\begin{align*}
N^2 E\left[ (Y_N(t+1/N)-Y_N(t))^2 | Y_N(t)=y\right] &=N^2 E\left[ (N^{-1} X_{[N(t+1/N)]}-N^{-1} X_{[Nt]})^2 | N^{-1} X_{[Nt]}=y\right] \\
&=  E\left[ (X_{[Nt]+1}-X_{[Nt]})^2 | X_{[Nt]}=i\right] \\
&= P\left( (X_{[Nt]+1}-X_{[Nt]})^2=1| X_{[Nt]}=i\right) \\
&=P\left( X_{[Nt]+1}-X_{[Nt]}=1 \bigcup X_{[Nt]+1}-X_{[Nt]}=-1 | X_{[Nt]}=i\right) \\
&=P\left( X_{[Nt]+1}-X_{[Nt]}=1| X_{[Nt]}=i\right)+P\left( X_{[Nt]+1}-X_{[Nt]}=-1| X_{[Nt]}=i\right) \\
&= i(u+v)
\end{align*}
The expected squared value of the difference is :
$$ E\left[ (Y_N(t+1/N)-Y_N(t))^2 | Y_N(t)=y\right]=\dfrac{y(u+v)}{N}
$$
\subparagraph{2)}
For $\alpha>\beta$ and $\alpha+\beta<1$, we have :

$$\displaystyle \left \{
    \begin{array}{l}
        E\left[ Y_N(t+1/N)-Y_N(t) | Y_N(t)=y\right]=\dfrac{y(\beta-\alpha)}{N} \\
		E\left[ (Y_N(t+1/N)-Y_N(t))^2 | Y_N(t)=y\right]=\dfrac{y(\alpha+\beta)}{N^2} \\
    \end{array}
\right.
$$
We set $h \sim 1/N$ and approximate the equations, for large but finite $N$ to :
$$
\left \{
\begin{array}{l}
        E\left[ Y_N(t+h)-Y_N(t) | Y_N(t)=y\right]=y(\beta-\alpha)h +o(h) \\
		E\left[ (Y_N(t+h)-Y_N(t))^2 | Y_N(t)=y\right]=\left(y\dfrac{(\alpha+\beta)}{N}\right)h +o(h) \\
    \end{array}
\right. 
$$
Moreover, to be a diffusion process, a Markov process $X(t), t\geq 0$ must be continuous and satisfies the following relations :\\

(1) $P\left( |Y_N(t+h)-Y_N(t)|>\epsilon | Y_N(t)\right)=o(h)$, for $\epsilon>0$ \\

(2) $E\left[ X(t+h)-X(t) | X(t)\right]=a(t,X(t))h +o(h)$ \\

(3) $E\left[( X(t+h)-X(t))^2 | X(t)\right]=b(t,X(t))h +o(h)$\\



For large but finite N, the state space of $Y_N(t)$ tend to be continuous and the requirements (2) and (3) are fulfilled for $a(t,Y_N(t))=Y_N(t)(\beta-\alpha)$ and $b(t,Y_N(t))=\left(Y_N(t)\dfrac{(\alpha+\beta)}{N}\right)$. We can also prove (1) by the definition of the process. \\

Thus in first approximation for large but finite $N$, even if it is not rigorous, our process is a diffusion process and is the solution of the stochastic differential equation : 
$$
dY_N=Y_N(\beta-\alpha)dt +\left(Y_N\dfrac{(\alpha+\beta)}{N}\right)^{1/2}dW
$$
Where $dW$ is the increment of the standard Wiener process.

\subparagraph{3)}
For $h \sim 1/N$ and $N \rightarrow \infty $, we denote $y(t)$ the limit of the process $Y_N(t)$, and the set of equations become 
$$
\left \{
\begin{array}{l}
        E\left[ y(t+h)-y(t) | y(t)\right]=y(t)(\beta-\alpha)h +o(h) \\
		E\left[ (y(t+h)-y(t))^2 | y(t)\right]=y(t)(\alpha+\beta)h^2 +o(h^2) \\
    \end{array}
\right. 
$$

$$
\iff
\left \{
\begin{array}{l}
        E\left[ y(t+h)-y(t) | y(t)\right]=y(t)(\beta-\alpha)h +o(h) \\
		E\left[ (y(t+h)-y(t))^2 | y(t)\right]=o(h) \\
    \end{array}
\right. 
$$
And the process $y(t)$ is solution of the stochastic differential equation :
$$
dy(t)=y(t)(\beta-\alpha)dt
$$
Which is not stochastic but deterministic since $a(t,y(t))=0$, and computing the solution is straightforward.
$$y(t)=y(0)e^{(\beta-\alpha)t}$$

\subparagraph{4)}
\

Let $Z_N(t)=\sqrt{N}(Y_N(t)-y(t))$, thus $Z_N(t)=z\iff Y_N(t)=y(t)+z/\sqrt{N}$\\

We first compute the exact value of the following expectation 
\setlength{\jot}{8pt}
\begin{align*}
E\left[ Z_N(t+1/N)-Z_N(t) | Z_N(t)=z\right]&=E\left[ \sqrt{N}(Y_N(t+1/)-y(t+1/N))-(\sqrt{N}(Y_N(t)-y(t))) | Z_N(t)=z\right] \\
&=\sqrt{N}E\left[ (Y_N(t+1/N)-Y_N(t) | Z_N(t)=z\right]-\sqrt{N}(y(t+1/N)-y(t)) \\
&=\sqrt{N}E\left[ (Y_N(t+1/N)-Y_N(t) | Y_N(t)=y(t)+z/\sqrt{N}\right]-\sqrt{N}(y(t+1/N)-y(t))
\end{align*}

By denoting $Z(t)$ the limit process of $Z_N(t)$, for $h \sim 1/N$ and  $N \rightarrow \infty $, this equation tend to
\setlength{\jot}{8pt}
\begin{align*}
E\left[ Z(t+h)-Z(t) | Z(t)=z\right] &=\dfrac{1}{\sqrt{h}}\left(y(t)-z\sqrt{h}\right)(\beta-\alpha)h-\dfrac{1}{\sqrt{h}}y(t)(\beta-\alpha)h+o(h) \\
&=z(\beta-\alpha)h +o(h)
\end{align*}

We also compute the exact value of the following squared expectation 
\setlength{\jot}{8pt}
\begin{align*}
E[ (Z_N(t+1/N)&-Z_N(t))^2  | Z_N(t)=z ]=E\left[ \left(\sqrt{N}(Y_N(t+1/N)-y(t+h))-\sqrt{N}(Y_N(t)-y(t))\right)^2 | Z_N(t)=z\right] \\
=&NE\left[ \left(Y_N(t+1/N)-y(t+1/N)-Y_N(t)+y(t)\right)^2 | Y_N(t)=y(t)+z/\sqrt{N}\right] \\
=&NE\left[ \left(Y_N(t+1/N)-Y_N(t)\right)^2 | Y_N(t)=y(t)+z/\sqrt{N}\right]+ N(y(t+1/N)-y(t))^2 \\
 & \quad -2N(y(t+1/N)-y(t))E\left[ Y_N(t+1/N)-Y_N(t)| Y_N(t)=y(t)+z/\sqrt{N}\right] \\
=&N(y(t)+z/\sqrt{N})\dfrac{\alpha+\beta}{N^2}+ N(y(t+1/N)-y(t))^2 -2N(y(t+1/N)-y(t))(y(t)+z/\sqrt{N})\dfrac{\beta-\alpha}{N}\\
=&(y(t)+z/\sqrt{N})\dfrac{\alpha+\beta}{N}+ N(y(t+1/N)-y(t))^2  -2(y(t+1/N)-y(t))(y(t)+z/\sqrt{N})(\beta-\alpha) 
\end{align*}

And again we take the limit of the equation for $N \rightarrow \infty $ and get
\setlength{\jot}{8pt}
\begin{align*}
E\left[ (Z(t+h)-Z(t))^2 | Z(t)=z\right]=& (y(t)+z\sqrt{h})(\alpha+\beta)h+y^2(t)(\beta-\alpha)^2 h -2y(t)(\beta-\alpha)h(y(t)+z\sqrt{h})(\beta-\alpha) +o(h) \\
=& y(t)(\alpha+\beta)h+ y^2(t)(\beta-\alpha)^2 h -2y^2(t)(\beta-\alpha)^2 h +o(h)\\
=& y(t)(\alpha+\beta)h-y^2(t)(\beta-\alpha)^2 h +o(h)
\end{align*}

And the process $Z(t)$ is solution of the stochastic differential equation :
\begin{align*}
dZ(t)=&Z(t)(\beta-\alpha)dt+(y(t)(\alpha+\beta)-y^2(t)(\beta-\alpha)^2)dW \\
=& a(Z(t),t)dt+b(t)dW
\end{align*}

















\paragraph{Exercise V.\\}

In the $M/M/\infty$ model, customers arrive for service at the instants of a Poisson process of rate $\lambda$. Service times start immediately on arrival, are independent and Exponential ($\mu$). $X(t)\in \mathbb{N}$ is the number of busy servers at time $t$. \\

If $i$ server are busy, the death rate from $i$ to $i-1$ is the rate at which the set of servers processes one of the client. Since the service are independent and exponential, this is $i\mu$. And the birth rate from $i$ to $i+1$ is $\lambda$, the rate at which customers arrive. \\

The infinitesimal generator of the chain is 
\begin{align*}
\mathbb{Q}=
\begin{pmatrix}
-\lambda & \lambda & 0 & 0 & 0 & 0 &\cdots  \\
\mu & -(\lambda+\mu) & \lambda & 0 & 0 & 0 & \\
0 & 2\mu & -(\lambda+2\mu) & \lambda  & 0 & 0 & \\
0 & 0 & 3\mu & -(\lambda+3\mu) & \lambda  & 0 &  \\
0 & 0 & 0 & 4\mu & -(\lambda+4\mu) & \lambda  &  \\
\vdots &  &  &  & \ddots & \ddots & \ddots  \\
\end{pmatrix}
\end{align*}

We denote $p_i(t)$ the probability $P(X(t)=i|X(0)=0)$, $\forall i \geq 0$. To get a relation between the $p_i(t)$ and their derivative, we use the Kolmogorov's forward equation $\mathbb{P}'=\mathbb{PQ}$, thus we get the following set of equations by reading only the first row of $\mathbb{P}'$
$$
\left \{
\begin{array}{l}
        p_0'(t)=\mu p_1(t)- \lambda p_0(t) \\
		p_i'(t)=\mu(i+1)p_{i+1}(t) -(\lambda+i\mu)p_i(t)+\lambda p_{i-1}(t) \qquad \forall i \geq 1 \\
    \end{array}
\right. 
$$

Since we can not find easily a solution to the set of equation, we first assume that $X(t)|X(0)=0$ is Poisson distributed, so that $p_i(t)=e^{-f_{\lambda,\mu}(t)}\dfrac{f_{\lambda,\mu}(t)^i}{i!}$, with $f_{\lambda,\mu}(t)$ a function we still have to determine.\\

If we can find a function $f_{\lambda,\mu}(t)$ such that $p_i(t)$ ($\forall i \geq 0$) fulfill the forward equation, then $X(t)|X(0)=0$ is Poisson distributed with mean $f_{\lambda,\mu}(t)$.\\

We first seek to find a function satisfying the first equality of the set of equations. \\
\begin{gather*} p_0'(t)=\mu p_1(t)- \lambda p_0(t) \\
\iff \left(e^{-f_{\lambda,\mu}(t)}\right)'=\mu e^{-f_{\lambda,\mu}(t)}f_{\lambda,\mu}(t)-\lambda e^{-f_{\lambda,\mu}(t)}\\
\iff  -f_{\lambda,\mu}'(t)e^{-f_{\lambda,\mu}(t)}=\mu e^{-f_{\lambda,\mu}(t)}f_{\lambda,\mu}(t)-\lambda e^{-f_{\lambda,\mu}(t)}\\
\iff f_{\lambda,\mu}'(t)=-\mu f_{\lambda,\mu}(t)+\lambda \\
\iff f_{\lambda,\mu}(t)=ke^{-\mu t} +\dfrac{\lambda}{\mu}, \quad k\in \mathbb{R} \\
\end{gather*}

The initial condition $p_0(0)=1$ leads to the value of $k$.
\begin{gather*} 
p_0(0)=1 \iff e^{- ke^{-\mu t} -\dfrac{\lambda}{\mu}}=e^0 \iff  k+\dfrac{\lambda}{\mu}=0 \iff k=-\dfrac{\lambda}{\mu}\\
\Rightarrow f_{\lambda,\mu}(t)=\dfrac{\lambda}{\mu}(e^{-\mu t}-1)
\end{gather*}
We verify that Kolmogorov's forward equation is fulfilled with this function $\forall i\geq 1$ :
\begin{align*}
p_i'(t) &=  \left(e^{-f_{\lambda,\mu}(t)}\dfrac{f_{\lambda,\mu}(t)^i}{i!}\right)' = e^{-f_{\lambda,\mu}(t)}\left(\dfrac{f_{\lambda,\mu}(t)^i}{i!}\right)'-f_{\lambda,\mu}'(t)e^{-f_{\lambda,\mu}(t)}\dfrac{f_{\lambda,\mu}(t)^i}{i!} \\
&= e^{-f_{\lambda,\mu}(t)} \left( f_{\lambda,\mu}'(t)\dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!}- f_{\lambda,\mu}'(t)\dfrac{f_{\lambda,\mu}(t)^i}{i!} \right) \\
&= e^{-f_{\lambda,\mu}(t)} \left( (\lambda-\mu f_{\lambda,\mu}(t)) \dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!}- (\lambda-\mu f_{\lambda,\mu}(t)) \dfrac{f_{\lambda,\mu}(t)^i}{i!} \right) \\
&= e^{-f_{\lambda,\mu}(t)} \left( \lambda\dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!}-\mu \dfrac{f_{\lambda,\mu}(t)^{i}}{(i-1)!}-\lambda\dfrac{f_{\lambda,\mu}(t)^{i}}{i!}+\mu \dfrac{f_{\lambda,\mu}(t)^{i+1}}{i!} \right)
\end{align*}

Moreover : 
\begin{align*}
\mu(i+1)p_{i+1}(t) -(\lambda+i\mu)p_i(t)+\lambda p_{i-1}(t) &= \mu(i+1)e^{-f_{\lambda,\mu}(t)}\dfrac{f_{\lambda,\mu}(t)^{i+1}}{(i+1)!} -(\lambda+i\mu)e^{-f_{\lambda,\mu}(t)}\dfrac{f_{\lambda,\mu}(t)^i}{i!}+\lambda e^{-f_{\lambda,\mu}(t)}\dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!} \\
&= e^{-f_{\lambda,\mu}(t)} \left( \mu(i+1) \dfrac{f_{\lambda,\mu}(t)^{i+1}}{(i+1)!} -\lambda\dfrac{f_{\lambda,\mu}(t)^i}{i!} -\mu i\dfrac{f_{\lambda,\mu}(t)^i}{i!}+\lambda \dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!} \right) \\
&= e^{-f_{\lambda,\mu}(t)} \left( \mu \dfrac{f_{\lambda,\mu}(t)^{i+1}}{i!} -\lambda\dfrac{f_{\lambda,\mu}(t)^i}{i!} -\mu \dfrac{f_{\lambda,\mu}(t)^i}{(i-1)!}+\lambda \dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!} \right) \\
&= e^{-f_{\lambda,\mu}(t)} \left( \lambda\dfrac{f_{\lambda,\mu}(t)^{i-1}}{(i-1)!}-\mu \dfrac{f_{\lambda,\mu}(t)^{i}}{(i-1)!}-\lambda\dfrac{f_{\lambda,\mu}(t)^{i}}{i!}+\mu \dfrac{f_{\lambda,\mu}(t)^{i+1}}{i!} \right) \\
&= p_i'(t) 
\end{align*}

Thus, with the Poisson distribution assumption for $X(t)|X(0)=0$ and $f_{\lambda,\mu}(t)=\dfrac{\lambda}{\mu}(e^{-\mu t}-1)$ the Kolmogorov's forward equation is satisfied.\\

$X(t)|X(0)=0$ is Poisson distributed with $P(X(t)=i|X(0)=0)=\dfrac{\lambda^i}{\mu^i}e^{-\lambda(e^{-\mu t}-1)/ \mu}\dfrac{\left(e^{-\mu t}-1\right)^i}{i!}$ and $E[X(t)|X(0)=0]=\dfrac{\lambda}{\mu}(e^{-\mu t}-1)$.







\newpage
\paragraph{Exercise VI.\\}
\subparagraph{1)}
\

We condition to the first jump to compute $m_n$.\\

We start the chain at state $n$, and we wait for the first jump to happens which take $\dfrac{1}{\lambda+\mu}$, the jump is either towards $n+1$ with probability $\dfrac{\lambda}{\lambda+\mu}$ or towards $n-1$ with probability $\dfrac{\mu}{\lambda+\mu}$. \\

If the first jump is toward $n+1$, we do not wait for reaching $n+1$. However, if the chain the first jump is toward $n-1$, we expect to wait $m_{n-1}+m_{n}$ to reach state $n+1$. To sum it up we have :

$$m_n=\dfrac{1}{\lambda+\mu}+0\dfrac{\lambda}{\lambda+\mu}+(m_{n-1}+m_{n})\dfrac{\mu}{\lambda+\mu}$$

$$\iff \lambda m_n+\mu m_n =1 + \mu (m_{n-1}+m_{n})$$
$$ \iff m_n=\dfrac{1}{\lambda}+\dfrac{\mu}{\lambda}m_{n-1}$$
\subparagraph{2)}
\
The fix point fulfill the equation $M=\dfrac{1}{\lambda}+\dfrac{\mu}{\lambda}M \iff M=\dfrac{1}{\lambda-\mu}$\\

\begin{align*}
m_n-M &= \dfrac{1}{\lambda}+\dfrac{\mu }{\lambda}m_{n-1}-\dfrac{1}{\lambda}-\dfrac{\mu}{\lambda}M\\
 &=\dfrac{\mu }{\lambda}(m_{n-1}-M) \\
 &=\left(\dfrac{\mu}{\lambda} \right)^n (m_{0}-M) \text{ with } m_0=\dfrac{1}{\lambda}
\end{align*}
$$\iff m_n=-\left(\dfrac{\mu}{\lambda} \right)^{n+1} \dfrac{1}{\lambda-\mu}+\dfrac{1}{\lambda-\mu}$$

\subparagraph{3)}
\
The expected time until the chain reaches n for the first time is : $$ \displaystyle \sum_{i=0}^{n-1}m_i=\sum_{i=0}^{n-1} \left[ \dfrac{1}{\lambda-\mu}-\left(\dfrac{\mu}{\lambda} \right)^{i+1} \dfrac{1}{\lambda-\mu} \right]=\dfrac{n}{\lambda-\mu}-\dfrac{1}{\lambda-\mu}\sum_{i=0}^{n-1} \left(\dfrac{\mu}{\lambda} \right)^{i+1}=\dfrac{n}{\lambda-\mu}-\dfrac{\mu}{(\lambda-\mu)^2} \left(1-\left(\dfrac{\mu}{\lambda} \right)^{n}\right) $$

\newpage
\appendix
\paragraph{Appendix 1}
\


\begin{lstlisting}
P=linspace(0.67,1,30)
lowerbound=zeros(30)
higherbound=zeros(30)
estimate=zeros(30)
for i=1:30 do 
    p=P(i)
    bootstrap=zeros(1,1000)
    for j=1:1000 do
        n=5000
        M=zeros(1,n)
        for k=1:n do
            if rand()<p then
                loop=%F
                M(k)=1
            else
                x=2
                T=1
                loop=%T
            end
            while loop do
                T=T+1
                if rand()<p then
                    x=x-1
                    if x==0 then
                        M(k)=T
                        loop=%F
                    end
                else x=x+2
                end
            end
        end
        bootstrap(j)=1/mean(M)
    end
    bootstrap=gsort(bootstrap)
    lowerbound(i)=bootstrap(975)
    higherbound(i)=bootstrap(25)
    estimate(i)=mean(bootstrap)
end
plot(P,higherbound,"red")
plot(P,lowerbound,"green")
plot(P,estimate)

\end{lstlisting}

\end{document}