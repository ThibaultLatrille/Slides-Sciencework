\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{fullpage}


\author{\and Susanne BornelÃ¶v \and Thibault Latrille \and Yiming Zhang}
\title{Computer-intensive statistics and data mining
 \\ Home assignment 3}  
\newtheorem*{theo}{Theorem} 
\begin{document}
\maketitle
\paragraph{Exercise 1.}
\subparagraph{a)}
Let $X$ a random variable from the Pareto distribution, characterized by a scale parameter $k>0$ and a shape parameter $\alpha>1$, with cdf $F_X(x)=1-(\frac{k}{x})^{\alpha}$. 
$$\text{The expecation of $X$ is then defined and equal  }E(X)=\dfrac{k\alpha}{\alpha-1}$$ $$\text{Furthermore } L(F_X)=\dfrac{E(X)}{E(X)-k}=\dfrac{k\alpha}{\alpha-1} \left( \dfrac{k\alpha}{\alpha-1}-k \right)^{-1}=\dfrac{k\alpha}{\alpha-1} \left( \dfrac{k}{\alpha-1} \right)^{-1}=\dfrac{k\alpha}{\alpha-1}  \dfrac{\alpha-1}{k}=\alpha$$ It follows the plug-in estimator of $\theta$ is $\widehat{\alpha}=L(F_n)$, where $F_n$ is the empirical distribution
function of the sample $\pmb{X}=(X_1,\ldots,X_n)$.
$$L(F_n)=\dfrac{\sum_{i=1}^n X_i}{\sum_{i=1}^n X_i-k}$$
\subparagraph{b)}
In a similar fashion,  $T(F_X)=\dfrac{Var(X)}{k^2}=\dfrac{\alpha}{(\alpha-1)^2(\alpha-2)}=\theta$. Hence $\widehat{\theta}=T(F_n)=\dfrac{1}{nk^2}\sum_{i=1}^n(X_i-\overline{X})^2$ is the plug-in estimator of $\theta$
\subparagraph{c)}
To generate the data, we use the classical inverse method since $F^-(x)=\dfrac{1}{(1-x)^{\frac{1}{\alpha}}}$

\begin{footnotesize}
\begin{verbatim}
k<-2
alpha<-3
data<-k/((1-runif(11))**(1/alpha))
\end{verbatim} 
\end{footnotesize}

For the non-parametric bootstrap, the bootstrap sample $\pmb{X}_j^*=(X_{1,j}^*,\ldots,X_{n,j}^*)$ for $j=1,\ldots,\beta$ is drawn from re-sampling the original data with replacement.\\


For each bootstrap sample replicate, we compute the plug-in estimator of $\theta$ described above, thus $\widehat{\theta}_j^*=T(F_{n,j}^*)$ where $F_{n,j}^*$ is the empirical distribution function of the bootstrap sample $\pmb{X}_j^*$. 
\\

Finally the percentile bootstrap 5\% confidence interval is given by $$\left[\widehat{\theta}^*_{[(1+\beta)(0.025)]} , \widehat{\theta}^*_{[(1+\beta)(0.975)]} \right]=[0.03,4.88]\text{ where $\widehat{\theta}^*_{[j]}$ is the $j^{th}$ ordered statistic.} $$
\begin{footnotesize}
\begin{verbatim}
theta<-function(x){
return((sum(x**2)/length(x)-mean(x)**2)/(k*k))} # plug-in estimator of theta

bootsample<-rep(XA,B)
for( i in 1:1000){bootsample[i]<-theta(sample(data,replace=T))} # estimation of theta in the bootstrap world
theta.hat<-theta(data) # estimation of theta in the real world
L1<-sort(bootsample)[25] # lower bound for the 5% confidence interval
U1<-sort(bootsample)[975] # upper bound for the 5% confidence interval
\end{verbatim}
\end{footnotesize}

\subparagraph{d)}
For the parametric bootstrap, the bootstrap sample $\pmb{X}_j^*=(X_{1,j}^*,\ldots,X_{n,j}^*)$ for $j=1,\ldots,\beta$ is drawn from the pareto distribution characterised by the scale parameter $\widehat{\alpha}$. $\widehat{\alpha}=L(F_{n})$ is the plug-in estimator of $\alpha$ defined in a). \\

As previously, for each bootstrap sample replicate, $\widehat{\theta}_j^*=T(F_{n,j}^*)$ where $F_{n,j}^*$ is the empirical distribution function of the bootstrap sample $\pmb{X}_j^*$. \\

Finally the percentile bootstrap 5\% confidence interval is given by $$\left[\widehat{\theta}^*_{[(1+\beta)(0.025)]} , \widehat{\theta}^*_{[(1+\beta)(0.975)]} \right]=[0.03,1.65]\text{ where $\widehat{\theta}^*_{[j]}$ is the $j^{th}$ ordered statistic.} $$
\begin{footnotesize}
\begin{verbatim}
ALPHA<-mean(data)/(mean(data)-k) # plug-in estimator of alpha, see a)
bootsample<-rep(XA,1000)
for( i in 1:1000){bootsample[i]<-theta(k/((1-runif(11))**(1/ALPHA)))} # estimation of theta in the bootstrap world
# the bootstrap sample replicate is drawn using an approximation of alpha
L2<-sort(bootsample)[25] # upper bound for the 5% confidence interval
U2<-sort(bootsample)[975] # upper bound for the 5% confidence interval
\end{verbatim}
\end{footnotesize}
\subparagraph{e)}
\
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.5\textwidth]{HA3_1.png}
  	\caption{\textbf{5 \% confidence interval for $\widehat{\theta}$.} \textsl{The parametric method (red segment) is preferred rather than the non-parametric method (green segment). The underlying value $0.75$ is contained in both interval.} \ }
	\end{figure}

\paragraph{Exercise 2.}
\
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.5\textwidth]{HA3_2.png}
  	\caption{\textbf{Plot of the data $\pmb{x_i=(A_i,B_i)}$ from the spatial data set.}}
	\end{figure}

\paragraph{Exercise 3.}
\subparagraph{a)}
Let F be the cdf of the random variable A. 
$\theta=T(F)=Var(A)=\int x^2dF(x)-(\int xdF(x))^2$. 
It follows the plug-in estimator $\widehat{\theta}=T(F_n)$, 
where $F_{n}$ is the empirical distribution function of the sample $\pmb{A}=(A_1,\ldots,A_{n})$.
$$T(F_n)=\frac{1}{n}\sum_{i=1}^{n} A_i^2 -\overline{\pmb{A}}^2$$
\subparagraph{b)}
For the non-parametric bootstrap, the bootstrap sample $\pmb{A}_j^*=(A_{1,j}^*,\ldots,A_{n,j}^*)$ for $j=1,\ldots,\beta$ is drawn from re-sampling the original data with replacement.\\

For each bootstrap sample replicate, we compute the plug-in estimator of $\theta$ described above, thus $\widehat{\theta}_j^*=T(F_{n,j}^*)$ where $F_{n,j}^*$ is the empirical distribution function of the bootstrap sample $\pmb{A}_j^*$. Finally the percentile bootstrap 5\% confidence interval is given by $$\left[\widehat{\theta}^*_{[(1+\beta)(0.025)]} , \widehat{\theta}^*_{[(1+\beta)(0.975)]} \right]=[88.4,279.7] \text{ where $\widehat{\theta}^*_{[j]}$ is the $j^{th}$ ordered statistic.}$$
\begin{footnotesize}
\begin{verbatim}
A<-c(48,36,20,29,42,42,20,42,22,41,45,14,6,0,33,28,34,4,32,24,47,41,24,26,30,41)
theta<-function(x){
return((sum(x**2)/length(x)-mean(x)**2))} # plug-in estimator of theta
bootsample<-rep(XA,1000)
for( i in 1:1000){bootsample[i]<-theta(sample(A,replace=T))} # estimation of theta in the bootstrap world
theta.hat<-theta(A) # estimation of theta in the real world
L1<-sort(bootsample)[25] # lower bound for the 5% confidence interval
U1<-sort(bootsample)[975] # upper bound for the 5% confidence interval
hist(bootsample)
points(theta.hat,0)
\end{verbatim}
\end{footnotesize}
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.4\textwidth]{HA3_3.png}
  	\caption{\textbf{Histogram of $\pmb{\widehat{\theta}^*}$ for the non-parametric bootstrap.} \textsl{The dot is $\widehat{\theta}$.}}
	\end{figure}
\subparagraph{c)}
For the parametric bootstrap, we assume $A \sim \mathcal{X}(\mu,\sigma^2)$. The bootstrap sample  $\pmb{A}_j^*=(A_{1,j}^*,\ldots,A_{n,j}^*)$ for $j=1,\ldots,\beta$ is drawn from the normal distribution $A^* \sim \mathcal{X}(\widehat{\mu},\widehat{\sigma}^2)$, where $\widehat{\mu}$ and $\widehat{\sigma}^2$ are the classical sample mean and variance. \\

For each bootstrap sample replicate, $\widehat{\theta}_j^*=T(F_{n,j}^*)$ where $F_{n,j}^*$ is the empirical distribution function of the bootstrap sample $\pmb{A}_j^*$. Finally the percentile bootstrap 5\% confidence interval is given by $$\left[\widehat{\theta}^*_{[(1+\beta)(0.025)]} , \widehat{\theta}^*_{[(1+\beta)(0.975)]} \right]=[90.9,271.0]\text{ where $\widehat{\theta}^*_{[j]}$ is the $j^{th}$ ordered statistic.} $$
 
\begin{footnotesize}
\begin{verbatim}
mu=mean(A) # mean estimator
sigma=sqrt((sum(A**2)-mean(A)**2*26)/25) # variance estimator
bootsample=rep(XA,1000)
for( i in 1:1000){bootsample[i]<-theta(rnorm(26,mu,sigma))} # estimation of theta in the bootstrap world
# the bootstrap sample replicate is drawn using an approximation of the real mean and variance.
L2=sort(bootsample)[25] # upper bound for the 5% confidence interval
U2<-sort(bootsample)[975] # upper bound for the 5% confidence interval
\end{verbatim}
\end{footnotesize}
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.4\textwidth]{HA3_4.png}
  	\caption{\textbf{Histogram of $\pmb{\widehat{\theta}^*}$ for the parametric bootstrap}. \textsl{The dot is $\widehat{\theta}$.}}
	\end{figure}
\subparagraph{d)}

Define $Z=\dfrac{\widehat{\theta}-\theta}{\sqrt{\widehat{V}}}$, where $\widehat{V}$ is an estimator of $Var(\widehat{\theta})$. The plug-in 
estimator of $\widehat{V}$ is $$\widehat{V}=V(F_n)=\dfrac{n-1}{n^3} T(F_n)^2 \left((n-1) \dfrac{\sum_{i=1}^n(A_i-\overline{\pmb{A}})^4}{nT(F_n)^2}-n+3 \right) $$
Using the previous denotation, the bootstrap replications for $Z$ are $Z^*_j=\dfrac{\widehat{\theta}^*_j-\widehat{\theta}}{\sqrt{V(F^*_{n,j})}}$. 
Finally the studentized bootstrap 5\% confidence interval is given by 
$$\left[ \widehat{\theta}-\sqrt{\widehat{V}}Z^*_{[(1+\beta)(0.975)]} , \widehat{\theta}-\sqrt{\widehat{V}}Z^*_{[(1+\beta)(0.025)]} \right]=[100.7,349.6] \text{ where $\widehat{Z}^*_{[j]}$ is the $j^{th}$ ordered statistic of $Z$ .}$$
\begin{footnotesize}
\begin{verbatim}
V<-function(x){n<-length(x)
return((n-1)/n**3*T(x)**2*((n-1)/n*sum((x-mean(x))**4)/T(x)**2-n+3))}
bootsample<-rep(XA,1000)
for( i in 1:1000){
replicate<-sample(A,replace=TRUE)
bootsample[i]<-(T(replicate)-theta.hat)/sqrt(V(replicate))}
bootsample<-sort(theta.hat-sqrt(V(A))*bootsample)
L3<-bootsample[25]
U3<-bootsample[975]
\end{verbatim}
\end{footnotesize}

\paragraph{Exercise 4.}
\subparagraph{a)}
\begin{footnotesize}
\begin{verbatim}
A=c(48,36,20,29,42,42,20,42,22,41,45,14,6,0,33,28,34,4,32,24,47,41,24,26,30,41)
B=c(42,33,16,39,38,36,15,33,20,43,34,22,7,15,34,29,41,13,38,25,27,41,28,14,28,40)
plot(A,B,main="Spatial data set:",xlab="A",
ylab="B")
model=lm(A~B)
plot(B,A)
abline(model)
\end{verbatim}
\end{footnotesize}
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.4\textwidth]{HA3_6.png}
  	\caption{\textbf{Least squares line fitted to the data}}
	\end{figure}

\subparagraph{b)}
For the non-parametric bootstrap of the least squares estimator of b, the bootstrap sample $\pmb{X}_j^*=(X_{1,j}^*,\ldots,X_{n,j}^*)=\left( (A_{1,j}^*,B_{1,j}^*),\ldots,(A_{n,j}^*,B_{n,j}^*) \right) $ for $j=1,\ldots,\beta$ is drawn from re-sampling the original paired data with replacement.
For each bootstrap sample replicate, $\widehat{b}^*_j$ is the replicate least squares estimate of b.

\begin{footnotesize}
\begin{verbatim}
bootsample<-rep(XA,1000)
for( i in 1:1000){
replicate<-sample(1:26, replace=TRUE)
As=A[replicate]
Bs=B[replicate]
bootsample[i]<-coef(lm(As~Bs))[2]}
\end{verbatim}
\end{footnotesize}

\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.4\textwidth]{HA3_7.png}
  	\caption{\textbf{Histogram of $\pmb{\widehat{b}^*}$} for the non-parametric bootstrap.}
	\end{figure}
\subparagraph{c)}
Finally the percentile bootstrap 5\% confidence interval is given by $$\left[\widehat{b}^*_{[(1+\beta)(0.025)]} , \widehat{b}^*_{[(1+\beta)(0.975)]} \right]=[0.74,1.31]\text{ where $\widehat{b}^*_{[j]}$ is the $j^{th}$ ordered statistic.} $$
 
\begin{footnotesize}
\begin{verbatim}
L<-sort(bootsample)[25] # upper bound for the 5% confidence interval
U<-sort(bootsample)[975] # upper bound for the 5% confidence interval
\end{verbatim}
\end{footnotesize}
\subparagraph{d)}
Since we previously computed the confidence interval, the test is straightforward : we reject the null hypothesis if b is outside the confidence interval. Here we accept the null hypothesis ($b=1$) at level 5\%.

\paragraph{Exercise 5.}
\subparagraph{a)}
 $ $
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.5\textwidth]{HA3_10.png}
  	\caption{\textbf{Several fitting procedures for the data}. \textsl{By symmetry of the errors-in-variables, both the naive least squares estimate of $B=f(A)$ (red) and $A=f(B)$ (black) are admissible. The bisection of least square estimates is a better approximation, since it does not favour $A$ nor $B$. The total least square (green) is also a better estimate, assuming the variance of errors-in-variables are equal. The SIMEX procedure (cyan) is based on extrapolation. The data are centred by the means for simplicity, which does not affect the result since every estimated line pass trough the means.}}
	\end{figure}
	
\subparagraph{b)}
\begin{footnotesize}
\begin{verbatim}
A<-c(48,36,20,29,42,42,20,42,22,41,45,14,6,0,33,28,34,4,32,24,47,41,24,26,30,41)
B<-c(42,33,16,39,38,36,15,33,20,43,34,22,7,15,34,29,41,13,38,25,27,41,28,14,28,40)
n<-length(A)
Replicate<-30
Nlambda<-5
NORMA=array(rnorm(Replicate*Nlambda*n,0,6),c(Replicate,Nlambda,n)) #generation of errors
NORMB=array(rnorm(Replicate*Nlambda*n,0,6),c(Replicate,Nlambda,n)) #generation of errors

SIMEX<-function(A,B,Replicate,Nlambda,NORMA,NORMB){
b_estimate1<-array(NA,c(Replicate,Nlambda)) #initialisation of variables
b_estimate2<-array(NA,c(Replicate,Nlambda)) #initialisation of variables
LAMBDA<-rep(seq(Nlambda),rep(Replicate,Nlambda)) #initialisation of variables
for (lambda in seq(Nlambda)){
for ( i in seq(Replicate)){

BS<-B+sqrt(lambda)*NORMA[i,lambda,]
model<-lm(A~BS)
b_estimate1[i,lambda]<-coef(model)[2]

AS<-A+sqrt(lambda)*NORMB[i,lambda,]
model<-lm(B~AS)
b_estimate2[i,lambda]<-1/(coef(model)[2])
	}
}
model1<-lm(c(b_estimate1)~LAMBDA)
model2<-lm(c(b_estimate2)~LAMBDA)
c1=coef(model1)
c2=coef(model2)
return(((c2[1]-c1[1])/(c1[2]-c2[2]))*c2[2]+c2[1])}

SIMEX(A,B,Replicate,Nlambda,NORMA,NORMB)
\end{verbatim}
\end{footnotesize}
\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.5\textwidth]{HA3_8.png}
  	\caption{\textbf{The SIMEX procedure.} \textsl{The dots are the estimates of b for different sample generation and different $\lambda$. The green line is the extrapolate curve for b when we add noise to A add fit the least square $B=f(A)$. On the other hand, the black line is the extrapolate curve for b when we add noise to B add fit the least square $A=f(B)$. The intersection of these two lines gives the unbiased estimator of b. Here $\widehat{b}=0.97$}}
	\end{figure}

\subparagraph{c)}
Computing the bootstrap interval is straightforward, regarding to the previous exercise. 

\begin{footnotesize}
\begin{verbatim}
bootsample<-rep(NA,1000)
for( i in 1:1000){
replicate<-sample(1:26, replace=TRUE)
As=A[replicate]
Bs=B[replicate]
bootsample[i]<-SIMEX(As,Bs,Replicate,Nlambda,NORMA,NORMB)}
hist(bootsample, xlab="b*", main="Histogram of the bootstrap replicate estimation of b")
L1=sort(bootsample)[25]
U1=sort(bootsample)[975]
\end{verbatim}
\end{footnotesize}

\begin{figure}[H]
	  \centering
  	\includegraphics[width=0.4\textwidth]{HA3_9.png}
  	\caption{\textbf{Histogram of  $\pmb{\widehat{b}^*}$ for the non-parametric SIMEX bootstrap.} \textsl{We get the percentile bootstrap 5\% confidence interval equal $[0.607,1.248]$, which include $1$.}}
	\end{figure}
	
	

\paragraph{Exercise 6.}
\subparagraph{a)}

Let $\pmb{X}=(X_1,X_2,X_3,X_4,X_5,X_6)$ where $X_1,X_2,X_3,X_4,X_5$ and $X_6$ are respectively the number of elements with genotype II, SS, FF, SI, SF and IF with the relation $\sum_{i=1}^6X_i=n$. 
$$\pmb{X} \sim M_6(X,\theta_1^2,\theta_2^2,\theta_3^2,2 \theta_1 \theta_2,2\theta_1\theta_3,2\theta_2\theta_3) \text{ where  $\theta_1+\theta_2+\theta_3=1$} $$ 

$$ P(\pmb{X}=\pmb{x}|\theta_1,\theta_2,\theta_3)=\dfrac{n!}{x_1!x_2!x_3!x_4!x_5!x_6!}(\theta_1^2)^{x_1}(\theta_2^2)^{x_2}(\theta_3^2)^{x_3}(2 \theta_1 \theta_2)^{x_4}(2\theta_1\theta_3)^{x_5}(2\theta_2\theta_3)^{x_6}$$\\

The log likelihood function up to an additive constant is :

$$l(\pmb{x}|\theta_1,\theta_2,\theta_3)=ln(P(\pmb{X}=\pmb{x}|\theta_1,\theta_2,\theta_3))=\sum_{i=1}^3 2x_i ln(\theta_i) + x_4ln(2\theta_1 \theta_2)+x_5 ln(2\theta_1 \theta_3)+x_6 ln(2\theta_2 \theta_3)$$

The task is to find the maximum of the log-likelihood with respect to $\theta_1,\theta_2,\theta_3$ under the constrain $\theta_1+\theta_2+\theta_3=1$. Thus, a lagrangian formulation is more appropriate :
$$\mathcal{L}(\pmb{x}|\theta_1,\theta_2,\theta_3,\lambda)=l(\pmb{x}|\theta_1,\theta_2,\theta_3)+\lambda(\theta_1+\theta_2+\theta_3-1)$$
The maximum likelihood estimators satisfy the equations :
$$ \forall i \in \{1,2,3\}, \quad \left. \dfrac{\partial\mathcal{L}}{\partial \theta_i} \right|_{\widehat{\theta}_1 , \widehat{\theta}_2,\widehat{\theta}_3,\widehat{\lambda}} =0 \quad \text{and} \quad
\left. \dfrac{\partial\mathcal{L}}{\partial \lambda} \right|_{\widehat{\theta}_1 , \widehat{\theta}_2,\widehat{\theta}_3,\widehat{\lambda}}=0$$
$$\iff$$

$$ \forall i \in \{1,2,3\}, \quad \dfrac{x_i^{'}}{\widehat{\theta}_i}+{\widehat{\lambda}}=0 \quad \text{and} \quad 
 \widehat{\theta}_1 + \widehat{\theta}_2 + \widehat{\theta}_3=1$$
\begin{center}
where $x_1^{'}=2x_1+x_4+x_5, \quad x_2^{'}=2x_2+x_4+x_6, \quad x_3^{'}=2x_3+x_5+x_6$ 
\end{center}
$$\iff$$

$$\widehat{\theta}_1(1+\dfrac{x_2^{'}}{x_1^{'}}+\dfrac{x_3^{'}}{x_1^{'}})=1,\quad \widehat{\theta}_2(1+\dfrac{x_1^{'}}{x_2^{'}}+\dfrac{x_3^{'}}{x_2^{'}})=1,
\quad \widehat{\theta}_3(1+\dfrac{x_2^{'}}{x_3^{'}}+\dfrac{x_1^{'}}{x_3^{'}})=1 $$
$$\iff$$

$$\widehat{\theta}_1=\dfrac{2x_1+x_4+x_5}{2n},\quad \widehat{\theta}_2=\dfrac{2x_2+x_4+x_6}{2n},
\quad \widehat{\theta}_3=\dfrac{2x_3+x_5+x_6}{2n} $$
\subparagraph{b)}
For the model (3), Let $Y_1=X_1+X_2+X_3$. Then $\pmb{Y}=(Y_1,X_4,X_5,X_6)$ is also multinomially distributed.
$$ P(\pmb{Y}=\pmb{y}|\theta_1,\theta_2,\theta_3)=\dfrac{n!}{y_1!x_4!x_5!x_6!}(\theta_1^2+\theta_2^2+\theta_3^2)^{y_1}(2 \theta_1 \theta_2)^{x_4}(2\theta_1\theta_3)^{x_5}(2\theta_2\theta_3)^{x_6}$$
The conditional distribution of $(X_1,X_2,X_3)/Y_1=y_1$ is :
$$ (X_1,X_2,X_3)/Y_1=y_1 \sim M_3(y_1,\dfrac{\theta_1^2}{\theta_1^2+\theta_2^2+\theta_3^2},\dfrac{\theta_2^2}{\theta_1^2+\theta_2^2+\theta_3^2},\dfrac{\theta_3^2}{\theta_1^2+\theta_2^2+\theta_3^2}) $$ 
Algorithm : \\
Current state : $\theta_1^{(k)},\theta_2^{(k)},\theta_3^{(k)}$\\
(E) Estimate the latent variables $x_1^{(k)}$, $x_2^{(k)}$, $x_3^{(k)}$ :
$$x_1^{(k)}=E(X_1/Y_1=y_1,\theta_1^{(k)},\theta_2^{(k)},\theta_3^{(k)})=y_1\dfrac{(\theta_1^{(k)})^2}{(\theta_1^{(k)})^2+(\theta_2^{(k)})^2+(\theta_3^{(k)})^2}$$
$$x_2^{(k)}=E(X_2/Y_1=y_1,\theta_1^{(k)},\theta_2^{(k)},\theta_3^{(k)})=y_1\dfrac{(\theta_2^{(k)})^2}{(\theta_1^{(k)})^2+(\theta_2^{(k)})^2+(\theta_3^{(k)})^2}$$
$$x_3^{(k)}=E(X_3/Y_1=y_1,\theta_1^{(k)},\theta_2^{(k)},\theta_3^{(k)})=y_1\dfrac{(\theta_3^{(k)})^2}{(\theta_1^{(k)})^2+(\theta_2^{(k)})^2+(\theta_3^{(k)})^2}$$
(M) Compute the maximum likelihood estimation $\theta_1^{(k+1)},\theta_2^{(k+1)},\theta_3^{(k+1)}$ in model (2) with
the estimated latent variables :
$$\theta_1^{(k+1)}=\dfrac{2x_1^{(k)}+x_4+x_6}{2n},
\quad \theta_2^{(k+1)}=\dfrac{2x_2^{(k)}+x_4+x_6}{2n},
\quad \theta_3^{(k+1)}=\dfrac{2x_3^{(k)}+x_5+x_6}{2n}$$
\begin{footnotesize}
\begin{verbatim}
y1=164;x4=41;x5=35;x6=160;n=400
T1=0.33;T2=0.33;T3=0.34;eps=10
while (eps>0.0001){
S=T1**2+T2**2+T3**2
x1=y1*T1**2/S; x2=y1*T2**2/S; x3=y1*T3**2/S;
NT1=(2*x1+x4+x5)/(2*n); NT2=(2*x2+x4+x6)/(2*n); NT3=(2*x3+x6+x5)/(2*n)
eps=max(c(NT1-T1,NT2-T2,NT3-T3))
T1=NT1; T2=NT2; T3=NT3}
\end{verbatim}
\end{footnotesize}
\subparagraph{c)}

We get $\widehat{\theta}_1=0.10,\quad \widehat{\theta}_2=0.48,
\quad \widehat{\theta}_3=0.42 $.

\end{document}
